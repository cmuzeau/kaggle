{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chest X-ray pneumonia\n",
    "---\n",
    "Read [homepage](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia) for more information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-04T17:18:33.680987Z",
     "iopub.status.busy": "2023-01-04T17:18:33.680533Z",
     "iopub.status.idle": "2023-01-04T17:18:42.332554Z",
     "shell.execute_reply": "2023-01-04T17:18:42.331362Z",
     "shell.execute_reply.started": "2023-01-04T17:18:33.680915Z"
    }
   },
   "outputs": [],
   "source": [
    "from   datetime import datetime\n",
    "import os\n",
    "from   pathlib import Path\n",
    "import time\n",
    "import random\n",
    "\n",
    "from   IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "from   mpl_toolkits.axes_grid1 import ImageGrid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from   PIL import Image\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from   plotly.subplots import make_subplots\n",
    "\n",
    "import torch\n",
    "assert torch.cuda.device_count() == 1, \"Select GPU P100 in Settings > Accelerator from the right panel.\"\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from   torch.utils.data import DataLoader\n",
    "\n",
    "from   torchmetrics.classification import BinaryConfusionMatrix\n",
    "from   sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import torchvision\n",
    "from   torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms.functional as F\n",
    "import torchvision.transforms as T\n",
    "from   torchvision.utils import make_grid\n",
    "from   torchvision.io import read_image, ImageReadMode\n",
    "from   torchvision.models import efficientnet_b4, densenet121\n",
    "\n",
    "torch.use_deterministic_algorithms(True)\n",
    "\n",
    "pd.set_option('max_colwidth', 400)\n",
    "pd.set_option('display.max_rows', 50)\n",
    "\n",
    "!nvidia-smi -L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-04T17:18:42.335873Z",
     "iopub.status.busy": "2023-01-04T17:18:42.335429Z",
     "iopub.status.idle": "2023-01-04T17:18:42.343930Z",
     "shell.execute_reply": "2023-01-04T17:18:42.341619Z",
     "shell.execute_reply.started": "2023-01-04T17:18:42.335833Z"
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed=0):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    os.environ['CUBLAS_WORKSPACE_CONFIG'] = \":4096:8\"\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False # ajouter par moi voir si cest tjr deterministique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables and persistent files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-04T17:18:42.345772Z",
     "iopub.status.busy": "2023-01-04T17:18:42.345143Z",
     "iopub.status.idle": "2023-01-04T17:18:42.354394Z",
     "shell.execute_reply": "2023-01-04T17:18:42.353244Z",
     "shell.execute_reply.started": "2023-01-04T17:18:42.345734Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = Path('../input/chest-xray-pneumonia/chest_xray/chest_xray/')\n",
    "\n",
    "csv_path = Path(\"./chest_xray.csv\")\n",
    "\n",
    "# create pandas dataframe from the dataset. Process takes around 1 minute to complete so we store the dataframe into a csv file for future use\n",
    "if not csv_path.exists():\n",
    "    df = pd.DataFrame(((path, *path.parts[-3:], *read_image(str(path)).size()) for path in data_path.rglob(\"*.jpeg\")),\n",
    "                      columns=[\"path\", \"subset\", \"label\", \"filename\", \"channel\", \"height\", \"width\"])\n",
    "    df[\"aspect_ratio\"] = df[\"width\"] / df[\"height\"]\n",
    "    df.to_csv(csv_path, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-04T17:18:42.357872Z",
     "iopub.status.busy": "2023-01-04T17:18:42.356891Z",
     "iopub.status.idle": "2023-01-04T17:18:42.390895Z",
     "shell.execute_reply": "2023-01-04T17:18:42.390072Z",
     "shell.execute_reply.started": "2023-01-04T17:18:42.357846Z"
    }
   },
   "outputs": [],
   "source": [
    "# load dataset into pandas dataframe\n",
    "df = pd.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# total size of the dataset\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset distribution\n",
    "df.groupby([\"label\", \"subset\"]).size().unstack()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset distribution\n",
    "with pd.option_context('display.float_format', \"{:.2%}\".format):\n",
    "    print(df[\"subset\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class distribution\n",
    "with pd.option_context('display.float_format', \"{:.0%}\".format):\n",
    "    print(df[\"label\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a clear imbalance, a lot more samples with PNEUMONIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trianing set distribution\n",
    "with pd.option_context('display.float_format', \"{:.0%}\".format):\n",
    "    print(df[df[\"subset\"] == \"train\"][\"label\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training set distribution similar to whole dataset distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summary statistics on the images\n",
    "with pd.option_context('display.float_format', '{:g}'.format):\n",
    "    display(df[[\"height\", \"width\", \"aspect_ratio\"]].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some images are RGBs but the 3 channels are actually equal\n",
    "df[\"channel\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some images are Grayscale while models require 3 channels as input, this is fixed when loading the image with `read_image` and using parameter `ImageReadMode.RGB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = [T.Resize((224,224), antialias=True)(read_image(str(path), ImageReadMode.GRAY)) for path in df[\"path\"].sample(n=4)]\n",
    "grid = make_grid(imgs, nrow=4).permute(1,2,0)\n",
    "plt.figure(figsize=(25,25))\n",
    "plt.imshow(grid)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Datasets and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-01-04T17:18:42.392620Z",
     "iopub.status.busy": "2023-01-04T17:18:42.392255Z",
     "iopub.status.idle": "2023-01-04T17:18:45.292729Z",
     "shell.execute_reply": "2023-01-04T17:18:45.291728Z",
     "shell.execute_reply.started": "2023-01-04T17:18:42.392586Z"
    }
   },
   "outputs": [],
   "source": [
    "# torch.manual_seed(0)\n",
    "# seed = torch.Generator()\n",
    "# seed_everything()\n",
    "\n",
    "# def seed_worker(worker_id):\n",
    "#     worker_seed = torch.initial_seed() % 2**32\n",
    "#     np.random.seed(worker_seed)\n",
    "#     random.seed(worker_seed)\n",
    "\n",
    "# g = torch.Generator()\n",
    "# g.manual_seed(0)\n",
    "\n",
    "def check_file(path):\n",
    "    try:\n",
    "        Image.open(path)\n",
    "        return True\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "def loader(path):\n",
    "    return read_image(path, ImageReadMode.RGB)\n",
    "\n",
    "# https://tcapelle.github.io/pytorch/fastai/2021/02/26/image_resizing.html\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((224, 224), antialias=True),\n",
    "#     T.RandomAffine(degrees=15, translate=(.2, .2), scale=(.8, 1.2)),\n",
    "    T.ToTensor(),\n",
    "#     T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    T.Normalize((0.485), (0.229)),\n",
    "])\n",
    "\n",
    "eval_transform = T.Compose([\n",
    "    T.Resize((224, 224), antialias=True),\n",
    "    T.ToTensor(),\n",
    "#     T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "    T.Normalize((0.485), (0.229)),\n",
    "])\n",
    "\n",
    "train_data = ImageFolder(\n",
    "    root=data_path/\"train\",\n",
    "    transform=train_transform,\n",
    "#     is_valid_file=check_file,\n",
    "#     loader=loader,\n",
    ")\n",
    "train_loader = DataLoader(\n",
    "    dataset=train_data,\n",
    "    batch_size=32,\n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "#     worker_init_fn=seed_worker,\n",
    "#     generator=g,\n",
    ")\n",
    "\n",
    "val_data = ImageFolder(\n",
    "    root=data_path/\"val\",\n",
    "    transform=eval_transform,\n",
    "#     is_valid_file=check_file,\n",
    "#     loader=loader,\n",
    ")\n",
    "val_loader = DataLoader(\n",
    "    dataset=val_data,\n",
    "    batch_size=8,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    ")\n",
    "\n",
    "test_data = ImageFolder(\n",
    "    root=data_path/\"test\",\n",
    "    transform=eval_transform,\n",
    "#     is_valid_file=check_file,\n",
    "#     loader=loader,\n",
    ")\n",
    "test_loader = DataLoader(\n",
    "    dataset=test_data,\n",
    "    batch_size=32,\n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths = df[\"path\"].sample(n=24)\n",
    "trans = [train_transform(Image.open(path)) for path in paths]\n",
    "\n",
    "fig = plt.figure(figsize=(25., 25.))\n",
    "grid = ImageGrid(fig, 111,\n",
    "                 nrows_ncols=(3, 8),\n",
    "                 axes_pad=.1,\n",
    "                 )\n",
    "\n",
    "for ax, im in zip(grid, trans):\n",
    "    ax.imshow(im.permute((1,2,0)), cmap=\"gray\")\n",
    "    ax.axis(\"off\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-04T17:18:45.294926Z",
     "iopub.status.busy": "2023-01-04T17:18:45.294336Z",
     "iopub.status.idle": "2023-01-04T17:20:42.632420Z",
     "shell.execute_reply": "2023-01-04T17:20:42.631261Z",
     "shell.execute_reply.started": "2023-01-04T17:18:45.294890Z"
    }
   },
   "outputs": [],
   "source": [
    "seed_everything()\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "\n",
    "# for reproducibility, model must be run with the train loop because \n",
    "# the state of the model (weights) is changed after each iteration\n",
    "model = densenet121()\n",
    "model.classifier = nn.Sequential(\n",
    "    nn.Linear(1024,256),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256,32),\n",
    "    nn.Dropout(0.3),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(32,2),\n",
    "    nn.LogSoftmax(dim=1),\n",
    ")\n",
    "\n",
    "# model = efficientnet_b4()\n",
    "# model.classifier = nn.Sequential(\n",
    "#     nn.Linear(1792, 1024),\n",
    "#     nn.Dropout(0.3),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(1024, 256),\n",
    "#     nn.Dropout(0.3),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(256, 32),\n",
    "#     nn.Dropout(0.3),\n",
    "#     nn.ReLU(),\n",
    "#     nn.Linear(32,2),\n",
    "#     nn.LogSoftmax(dim=1),\n",
    "# )\n",
    "\n",
    "model.to(device)\n",
    "\n",
    "loss_fn = nn.NLLLoss()\n",
    "learning_rate = 1.\n",
    "optimizer = optim.Adadelta(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 1\n",
    "data = []\n",
    "for epoch in range(1, epochs+1):\n",
    "    train_loss = 0\n",
    "    nb_train_batches = len(train_loader)\n",
    "    train_acc = 0\n",
    "    model.train()\n",
    "    for batch, (image, target) in enumerate(train_loader, start=1):\n",
    "        print(f\"[Epoch {epoch:>2}] Train batch progress: {batch:>3}/{nb_train_batches}\", end='\\r')\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(image)\n",
    "        loss = loss_fn(output, target)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item() * image.size(0)\n",
    "        _, preds = torch.max(output, 1)\n",
    "        train_acc += (preds == target).float().sum().item()\n",
    "    \n",
    "    val_loss = 0\n",
    "    nb_val_batches = len(val_loader)\n",
    "    val_acc = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch, (image, target) in enumerate(val_loader, start=1):\n",
    "            print(f\"[Epoch {epoch:>2}] Val batch progress: {batch:>3}/{nb_val_batches}\", end='\\r')\n",
    "            image = image.to(device)\n",
    "            target = target.to(device)\n",
    "            output = model(image)\n",
    "            loss = loss_fn(output, target)\n",
    "            val_loss += loss.item() * image.size(0)\n",
    "            _, preds = torch.max(output, 1)\n",
    "            val_acc += (preds == target).float().sum().item()\n",
    "        \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    val_loss /= len(val_loader.dataset)\n",
    "    train_acc /= len(train_loader.dataset)\n",
    "    val_acc /= len(val_loader.dataset)\n",
    "    print(f\"[Epoch {epoch:>2}] Train loss: {train_loss:.5f} | Train accuracy: {train_acc:>7.2%} | Val loss: {val_loss:.5f} | Val accuracy: {val_acc:>7.2%}\")\n",
    "    data.append([epoch, learning_rate, train_loss, train_acc, val_loss, val_acc])\n",
    "    if epoch == 2:\n",
    "        break\n",
    "\n",
    "model_name = model._get_name() + '_' + datetime.now().strftime(\"%y%m%d-%H%M%S\")\n",
    "\n",
    "# save model summary\n",
    "df = pd.DataFrame(data, columns=[\"epoch\", \"learning_rate\", \"train_loss\", \"train_acc\", \"val_loss\", \"val_acc\"])\n",
    "Path(\"./runs\").mkdir(exist_ok=True)\n",
    "df.to_csv(\"./runs/\" + model_name + \".csv\", index=False)\n",
    "\n",
    "# save model\n",
    "# https://stackoverflow.com/questions/54746829/pytorch-whats-the-difference-between-state-dict-and-parameters\n",
    "Path(\"./models\").mkdir(exist_ok=True)\n",
    "# torch.save(model.state_dict(), \"./models/\" + model_name + '_weights_' + '.pth') # parameters only\n",
    "torch.save(model, \"./models/\" + model_name + '.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Epoch  1] Train loss: 0.27831 | Train accuracy:  88.32% | Val loss: 1.12007 | Val accuracy:  56.25%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# available runs\n",
    "runs = list(Path(\"./runs\").rglob(\"*.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose best performing model for test set\n",
    "summary = []\n",
    "for csv in runs:\n",
    "    epochs, *_, val_acc = pd.read_csv(csv).iloc[-1]\n",
    "    summary.append([csv.stem, int(epochs), val_acc])\n",
    "pd.DataFrame(summary, columns=[\"model\", \"epochs\", \"val_acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = make_subplots(rows=2, cols=2,\n",
    "                    shared_xaxes=True,\n",
    "                    vertical_spacing=0.02)\n",
    "\n",
    "nb_colors = len(px.colors.qualitative.Plotly)\n",
    "for i, run in enumerate(runs):\n",
    "    df = pd.read_csv(run)\n",
    "    color = px.colors.qualitative.Plotly[i%nb_colors]\n",
    "    fig.add_trace(go.Scatter(x=df[\"epoch\"], y=df[\"train_loss\"], name=run.stem + \".train_loss\", marker=dict(color=color), mode=\"lines\"),\n",
    "                  row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(x=df[\"epoch\"], y=df[\"val_loss\"], name=run.stem + \".val_loss\", marker=dict(color=color), mode=\"lines\"),\n",
    "                  row=2, col=1)\n",
    "    fig.add_trace(go.Scatter(x=df[\"epoch\"], y=df[\"train_acc\"], name=run.stem + \".train_acc\", marker=dict(color=color), mode=\"lines\"),\n",
    "                  row=1, col=2)\n",
    "    fig.add_trace(go.Scatter(x=df[\"epoch\"], y=df[\"val_acc\"], name=run.stem + \".val_acc\", marker=dict(color=color), mode=\"lines\"),\n",
    "                  row=2, col=2)\n",
    "\n",
    "fig.update_yaxes(title_text=\"train_loss\", row=1, col=1)\n",
    "fig.update_yaxes(title_text=\"val_loss\", row=2, col=1)\n",
    "fig.update_yaxes(title_text=\"train_acc\", row=1, col=2)\n",
    "fig.update_yaxes(title_text=\"val_acc\", row=2, col=2)\n",
    "fig.update_xaxes(title_text=\"epoch\", tick0=1, dtick=1, row=2, col=1)\n",
    "fig.update_xaxes(title_text=\"epoch\", tick0=1, dtick=1, row=2, col=2)\n",
    "fig.update_layout(hovermode=\"x unified\", margin=dict(l=0, r=0, t=0, b=0))\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load())\n",
    "model = torch.load('./models/DenseNet_230101-111116.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss = 0\n",
    "nb_test_batches = len(test_loader)\n",
    "\n",
    "metric = BinaryConfusionMatrix().to(device)\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for batch, (image, target) in enumerate(test_loader, start=1):\n",
    "        print(f\"Test batch progress: {batch:>3}/{nb_test_batches}\", end='\\r')\n",
    "        image = image.to(device)\n",
    "        target = target.to(device)\n",
    "        output = model(image)\n",
    "        loss = loss_fn(output, target)\n",
    "        test_loss += loss.item() * image.size(0)\n",
    "        _, preds = torch.max(output, 1)\n",
    "        metric.update(preds, target)\n",
    "\n",
    "clear_output()\n",
    "test_loss /= len(test_loader.dataset)\n",
    "conf_matrix = metric.compute().cpu()\n",
    "accuracy = conf_matrix.diag().sum() / conf_matrix.sum()\n",
    "print(f\"Test loss: {test_loss:.5f}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized = True\n",
    "\n",
    "conf_matrix_normalized = conf_matrix/conf_matrix.sum(1).reshape((2,1))\n",
    "\n",
    "fig = px.imshow(\n",
    "    conf_matrix_normalized if normalized else conf_matrix,\n",
    "    text_auto=\".2%\" if normalized else True,\n",
    "    labels=dict(x=\"<b>Prediction</b>\", y=\"<b>Ground Truth</b>\"),\n",
    "    x=train_data.classes,\n",
    "    y=train_data.classes,\n",
    "    title=\"<b>Confusion matrix</b>\",\n",
    "    width=700,\n",
    ")\n",
    "fig.update_xaxes(side=\"top\")\n",
    "# fig.update_coloraxes(showscale=False)\n",
    "fig.show()\n",
    "# confusion matrix shows True Positives (TP), False Positives (FP), True Negatives (TN) and False Negatives (FN)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
