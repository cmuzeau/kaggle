{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Chest X-ray pneumonia](https://www.kaggle.com/datasets/paultimothymooney/chest-xray-pneumonia)\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-05T15:44:40.466360Z",
     "iopub.status.busy": "2023-01-05T15:44:40.465221Z",
     "iopub.status.idle": "2023-01-05T15:44:40.471733Z",
     "shell.execute_reply": "2023-01-05T15:44:40.470812Z",
     "shell.execute_reply.started": "2023-01-05T15:44:40.466318Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import torch_xla.core.xla_model as xm\n",
    "\n",
    "start = time.time()\n",
    "device = xm.xla_device()\n",
    "print(f\"{time.time()-start:.0f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-05T15:44:45.301065Z",
     "iopub.status.busy": "2023-01-05T15:44:45.300356Z",
     "iopub.status.idle": "2023-01-05T15:44:45.306275Z",
     "shell.execute_reply": "2023-01-05T15:44:45.305290Z",
     "shell.execute_reply.started": "2023-01-05T15:44:45.301031Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch_xla\n",
    "# latest: '1.13'\n",
    "# pinned: '1.11'\n",
    "torch_xla.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-01-04T10:05:17.990713Z",
     "iopub.status.busy": "2023-01-04T10:05:17.990401Z",
     "iopub.status.idle": "2023-01-04T10:05:36.368064Z",
     "shell.execute_reply": "2023-01-04T10:05:36.366753Z",
     "shell.execute_reply.started": "2023-01-04T10:05:17.990646Z"
    }
   },
   "outputs": [],
   "source": [
    "from   collections import OrderedDict\n",
    "import os\n",
    "assert 'ISTPUVM' in os.environ, \"Select TPU 1VM v3-8 in Settings > Accelerator from the right panel.\"\n",
    "from   pathlib import Path\n",
    "import sys\n",
    "\n",
    "from   IPython.display import clear_output\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from   torch.utils.data import DataLoader\n",
    "from   torch.utils.data.distributed import DistributedSampler\n",
    "import torchvision\n",
    "from   torchvision import transforms as T, datasets\n",
    "from   torchvision.models import densenet121\n",
    "import torch_xla\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.test.test_utils as test_utils\n",
    "\n",
    "print(sys.version)\n",
    "print(f\"{torch.__version__=}\")\n",
    "print(f\"{torchvision.__version__=}\")\n",
    "print(f\"{torch_xla.__version__=}\")\n",
    "print(f\"{os.environ['KAGGLE_DOCKER_IMAGE']=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-31T17:38:48.434130Z",
     "iopub.status.busy": "2022-12-31T17:38:48.433664Z",
     "iopub.status.idle": "2022-12-31T17:47:18.716403Z",
     "shell.execute_reply": "2022-12-31T17:47:18.715131Z",
     "shell.execute_reply.started": "2022-12-31T17:38:48.434098Z"
    }
   },
   "outputs": [],
   "source": [
    "data_path = Path('../input/chest-xray-pneumonia/chest_xray/chest_xray')\n",
    "\n",
    "# https://pytorch.org/xla/release/1.13/index.html#torch_xla.distributed.xla_multiprocessing.MpSerialExecutor\n",
    "# https://discuss.pytorch.org/t/what-does-xmp-mpserialexecutor-really-do/96150\n",
    "# MX = xmp.MpModelWrapper(model)\n",
    "\n",
    "train_transform = T.Compose([\n",
    "    T.Resize((224, 224), antialias=True),\n",
    "    T.RandomAffine(degrees=15, translate=(0.2, 0.2), scale=(0.5, 1.5)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "eval_transform = T.Compose([\n",
    "    T.Resize((224,224), antialias=True),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225)),\n",
    "])\n",
    "\n",
    "train_data = datasets.ImageFolder(root=data_path/\"train\", transform=train_transform)\n",
    "val_data = datasets.ImageFolder(root=data_path/\"val\", transform=eval_transform)\n",
    "\n",
    "model = densenet121()\n",
    "model.classifier = nn.Sequential(OrderedDict([\n",
    "    ('fcl1', nn.Linear(1024,256)),\n",
    "    ('dp1', nn.Dropout(0.3)),\n",
    "    ('r1', nn.ReLU()),\n",
    "    ('fcl2', nn.Linear(256,32)),\n",
    "    ('dp2', nn.Dropout(0.3)),\n",
    "    ('r2', nn.ReLU()),\n",
    "    ('fcl3', nn.Linear(32,2)),\n",
    "    ('out', nn.LogSoftmax(dim=1)),\n",
    "]))\n",
    "\n",
    "def _mp_fn(index):\n",
    "    global model\n",
    "    \n",
    "    train_sampler = DistributedSampler(\n",
    "        train_data,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=True,\n",
    "    )\n",
    "    train_loader = DataLoader(\n",
    "        train_data,\n",
    "        batch_size=128,\n",
    "        sampler=train_sampler,\n",
    "        drop_last=False,\n",
    "        num_workers=8,\n",
    "    )\n",
    "    \n",
    "    val_sampler = DistributedSampler(\n",
    "        val_data,\n",
    "        num_replicas=xm.xrt_world_size(),\n",
    "        rank=xm.get_ordinal(),\n",
    "        shuffle=False\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_data,\n",
    "        batch_size=8,\n",
    "        sampler=val_sampler,\n",
    "        drop_last=False,\n",
    "        num_workers=8,\n",
    "    )\n",
    "\n",
    "    device = xm.xla_device()\n",
    "    train_device_loader = pl.MpDeviceLoader(train_loader, device)\n",
    "    val_device_loader = pl.MpDeviceLoader(val_loader, device)\n",
    "    \n",
    "    # model = MX.to(device)\n",
    "    model.to(device)\n",
    "    loss_fn = nn.NLLLoss()\n",
    "    optimizer = optim.Adadelta(model.parameters())\n",
    "    \n",
    "    epochs = 20\n",
    "    for epoch in range(1, epochs+1):\n",
    "        total = 0\n",
    "        total_samples = 0\n",
    "        model.train()\n",
    "        for step, (data, target) in enumerate(train_device_loader):\n",
    "            # print(f\"\\t[{xm.xla_real_devices([str(device)])[0]}] job started {step}\", flush=True)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = loss_fn(output, target)\n",
    "            loss.backward()\n",
    "            xm.optimizer_step(optimizer)\n",
    "            total += loss.item() * data.size(0)\n",
    "            total_samples += data.size(0)\n",
    "        train_loss_total = xm.mesh_reduce('total', total, np.sum)\n",
    "        train_loss_total_samples = xm.mesh_reduce('total_samples', total_samples, np.sum)\n",
    "        # xm.master_print(f\"[{epoch=:>2}] {device} {xm.xla_real_devices([str(device)])[0]} loss: {accuracy:.6f}\", flush=True)\n",
    "        \n",
    "        total = 0\n",
    "        total_samples = 0\n",
    "        acc = 0\n",
    "        acc_count = 0\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            for step, (data, target) in enumerate(val_device_loader):\n",
    "                output = model(data)\n",
    "                loss = loss_fn(output, target)\n",
    "                total += loss.item() * data.size(0)\n",
    "                total_samples += data.size(0)\n",
    "                _, preds = torch.max(output, 1)\n",
    "                acc += (preds == target).float().sum()\n",
    "                acc_count += data.size(0)\n",
    "        val_loss_total = xm.mesh_reduce('total', total, np.sum)\n",
    "        val_loss_total_samples = xm.mesh_reduce('total_samples', total_samples, np.sum)\n",
    "        val_acc = xm.mesh_reduce('acc', acc, np.sum)\n",
    "        val_acc_count = xm.mesh_reduce('acc_count', acc_count, np.sum)\n",
    "        \n",
    "        train_loss = train_loss_total / train_loss_total_samples\n",
    "        val_loss = val_loss_total / val_loss_total_samples\n",
    "        val_accuracy = val_acc / val_acc_count\n",
    "        xm.master_print(f\"[{epoch=:>2}] {train_loss=:.6f} | {val_loss=:.6f} | {val_accuracy=:.2%}\", flush=True)\n",
    "\n",
    "xmp.spawn(_mp_fn, args=(), nprocs=8, start_method='fork')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
